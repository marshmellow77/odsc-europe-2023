{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be28723-d8ea-4775-80e9-5cf3ffe03acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5356255-a2eb-47e2-838e-94a90044a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbc4d02-22ab-45a2-9e71-d3caa79eef16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"decapoda-research/llama-13b-hf\"\n",
    "kwargs = {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d550c4-9a47-42ab-a6aa-7a5f0737b433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch_dtype': torch.float16, 'device_map': 'auto'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca219d81-056a-4e92-b6a1-8a9a627209a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005313873291015625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 41,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d408cf9cea45ccabc9967d2c1bf8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    low_cpu_mem_usage=True, **kwargs, cache_dir=\"model_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087b745d-51bd-400e-ac38-bb1e433cc002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 14 17:19:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   32C    P0    60W / 300W |   6485MiB / 23028MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A10G         On   | 00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   33C    P0    60W / 300W |   7391MiB / 23028MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A10G         On   | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   32C    P0    59W / 300W |   7391MiB / 23028MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   33C    P0    62W / 300W |   6485MiB / 23028MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     41006      C   ...vs/pytorch_p39/bin/python     6483MiB |\n",
      "|    1   N/A  N/A     41006      C   ...vs/pytorch_p39/bin/python     7389MiB |\n",
      "|    2   N/A  N/A     41006      C   ...vs/pytorch_p39/bin/python     7389MiB |\n",
      "|    3   N/A  N/A     41006      C   ...vs/pytorch_p39/bin/python     6483MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8290fc-b640-493c-9374-0624f2e83645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52051afe-9bff-4501-aa38-fb576fa7f068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116a025f-3184-4afc-b4ea-01f1d48b9525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Answer the following question step by step:\n",
    "Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363612aa-509e-4c40-b7fc-8205e3178c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question step by step:\n",
      "Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
      "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Step 1: Write the problem in a word equation.\n",
      "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "Step 2: Solve the equation.\n",
      "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? 5 + 2 + 3 = 10\n",
      "Step 3: Write the answer in a word equation.\n",
      "Roger has 10 tennis balls.\n",
      "Step 4: Check your answer.\n",
      "Roger has 10 tennis balls. Roger has 10\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(inputs, max_new_tokens=200, temperature=0.1, do_sample=False)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef76da-887c-4271-b1b4-21ee2dbbd05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
